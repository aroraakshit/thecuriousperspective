---
layout: post
current: post
cover: 'assets/images/text-to-speech.jpg'
navigation: true
title: 'Mixer-TTS'
date: 2021-11-07 12:0:0
tags: DL Conversational-AI TTS Model-Architecture
class: post-template
subclass: 'post'
author: aroraakshit
---

# `Mixer-TTS: non-autoregressive, fast and compact text-to-speech model conditioned on language model embeddings` 
### by [Oktai Tatanov](https://ru.linkedin.com/in/oktai15), [Stanislav Beliaev](https://www.linkedin.com/in/stasbel), [Boris Ginsburg](https://www.linkedin.com/in/boris-ginsburg-2249545) (NVIDIA, Santa Clara).

Available on [arXiv](https://arxiv.org/abs/2110.03584) - Oct 2021. Code not available yet. 
This journal was last updated on 11/8/2021.

Note: The views and opinions expressed on this website are those of my own and do not represent those of my employer, NVIDIA.

## Abstract

This paper describes `Mixer-TTS`, a <span style="background-color: #FFFF00">[non-autoregressive](#tbd)</span> model for <span style="background-color: #FFFF00">[mel-spectrogram](#tbd)</span> generation. The idea here is that `Mixer-TTS` achieves similar quality (<span style="background-color: #FFFF00">measured as [Mean Opinion Score (MOS)](#tbd)</span>) as state of the art TTS models today, like [FastPitch](https://arxiv.org/abs/2006.06873), using almost half the parameters, and therefore better inference performance. Basic `Mixer-TTS` contains `pitch` and `duration` predictors as well.

## Notes

### Intuition behind Mixer-TTS:
1. Latest improvements in training and inference speed has been mostly related to switching from sequential, autoregressive models (like [Tacotron](https://arxiv.org/abs/1703.10135) - 2017, [Wavenet](https://arxiv.org/abs/1712.05884) - 2017 and [Deep Voice 3](https://arxiv.org/abs/1710.07654) - 2017) to parallel, non-autoregressive models (like [FastSpeech](https://arxiv.org/abs/1905.09263) - 2019, [FastSpeech 2](https://arxiv.org/abs/2006.04558) - 2020, [FastPitch](https://arxiv.org/abs/2006.06873) - 2020 and [Talknet](https://arxiv.org/pdf/2005.05514.pdf) - 2020).
    - Non-autoregressive models generate speech two order of magnitude faster than auto-regressive models with similar quality. 
    - <span style="background-color: #FFFF00">unknown meaning of autoregressive vs non-autoregressive</span>
    - Example, FastPitch generates mel-spectrograms 60x faster than Tacotron 2.
2. The robustness of TTS models has been improved by using an explicit duration predictor (like in [Deep Voice](https://arxiv.org/abs/1702.07825) - 2017, [`Deep Voice 2`](https://arxiv.org/abs/1705.08947) - 2017, [FastSpeech](https://arxiv.org/abs/1905.09263) - 2019, [FastSpeech 2](https://arxiv.org/abs/2006.04558) - 2020, [FastPitch](https://arxiv.org/abs/2006.06873) - 2020 and [Talknet](https://arxiv.org/pdf/2005.05514.pdf) - 2020)
    - Traditionally, models with duration predictors have been trained in a supervised manner with external ground truth alignments.
    - For example, TalkNet used the alignment from auxiliary ASR models, while FastSpeech and FastPitch used alignments from a teacher TTS model.
    - [Glow-TTS](https://arxiv.org/abs/2005.11129) - 2020 proposed a flow based algorithm for unsupervised alignment training, further improved in [RAD-TTS](https://openreview.net/pdf?id=0NQwnnwAORi) - 2021 and modified for non-autoregressive models [here - `One TTS Alignment To Rule Them All`](https://arxiv.org/abs/2108.10447) - 2021 - and that is used in Mixer-TTS model.
3. The quality of non-autoregressive models improved with:
    - [FastPitch](https://arxiv.org/abs/2006.06873) - 2020 adding a pitch predictor for <span style="background-color: #FFFF00">[fundamental frequency (F0)](#tbd)</span>. 
    - Further improvement suggested by [`Hayashi et al`](https://www.isca-speech.org/archive/interspeech_2019/hayashi19_interspeech.html) - 2019 was to augment TTS model with input representation from pre-trained [BERT](https://arxiv.org/abs/1810.04805) - 2018 language model.
        - Intuition here is that text embeddings contain information about the importance of each word, which helped to improve speech prosody and pronunciation.
    - The usage of semantic context for TTS was extended in [`Xu et al`](https://arxiv.org/abs/2011.05161) - 2021
4. The model backbone is based on [`MLP-Mixer`](https://arxiv.org/abs/2105.01601) - 2021 architecture from computer vision adapted for speech. 
    - MLP-Mixer makes the model significantly smaller and faster than Transformer-based TTS models (like [FastSpeech 2](https://arxiv.org/abs/2006.04558) - 2020, [FastPitch](https://arxiv.org/abs/2006.06873) - 2020) <span style="background-color: #FFFF00">not sure why, yet</span>
5. Using token embeddings is significantly less expensive inferring BERT outputs as shown in [`Hayashi et al`](https://www.isca-speech.org/archive/interspeech_2019/hayashi19_interspeech.html) - 2019. <span style="background-color: #FFFF00">Unclear intuition here</span>.
    - They notably improve speech quality with a very modest increase in the model size and inference speed.

There are two versions of Mixer-TTS presented in the paper:
1. `basic` - uses (<span style="background-color: #FFFF00">unclear about the implementation here - perhaps similar to Fastpitch</span>) some embeddings. Achieves a MOS of 4.05. About 19.2 M parameters.
2. `extended` - uses `token embeddings` from pre-trained language model (ALBERT from HuggingFace). Achieves a MOS of 4.11. About 24M parameters.

Samples published here: [mixer-tts.github.io](https://mixer-tts.github.io/)

### Architecture 
Here is what the model architecture looks like:

![figure:1](assets/images/2021-11-07-mixer-tts/f1.png)

We encode the text and align it by using audio features in a separate module to get "ground truth" durations. Then we calculate character or `phoneme-level` pitch values and feed them all into the length regulator module to expand each character or phoneme feature along with their corresponding durations. Next, the decoder generates mel-spectrogram from the encoded representations.

Mixer-TTS is structurally very similar to FastPitch with two major changes:
1. Replace all feed-forward Transformer-based blocks in the encoder and decoder with new Mixer-TTS blocks.
2. Use an unsupervised speech-to-text alignment framework to train duration predictor. However, duration and pitch predictor architectures are the same as FastPitch. 
3. The extended Mixer-TTS additionally includes conditioning on embeddings from pretrained LM.

Loss function - aligner loss and mean-squared errors between ground truth and predicted values for mel-spectrogram, duration and pitch: \\
L = L<sub>aligner</sub> + L<sub>mel</sub> + 0.1 * L<sub>durs</sub> + 0.1 * L<sub>pitch</sub>

#### Mixer-TTS block

![figure:2](assets/images/2021-11-07-mixer-tts/f2.png)

`MLP-Mixer` performs 2 key actions over input:
1. `mixing` the per-location features and
2. `mixing` spatial information

Both operations are performed by stack of two MLPs layers:
1. First MLP layer increases the number of channels by an `expansion factor`
2. Second MLP layer reduces the number to its original value

But such approach is only possible for when input size for a layer is fixed by every dimension. <span style="background-color: #FFFF00">Why?</span>

To use this architecture for TTS (input's dimensions have dynamic size), we use `time mixing`, replacing MLPs with <span style="background-color: #FFFF00">[depth-wise 1D convolutions](#tbd)</span> and borrowed the original layer for channel "mixing".
- During mini-batch training and inference, when sequences in a batch are padded to match the longest sequence, we use sequence masking after MLP and depth-wise 1D convolution layers.

Other model details:
- The encoder is composed of 6 stacked Mixer-TTS blocks with convolution kernel in time-mix growing linearly from 11 to 21 with step 2. <span style="background-color: #FFFF00">how were these numbers chosen? Or any of these below.</span>
- The decoder is composed of 9 stacked Mixer-TTS blocks with kernel sizes growing from 15 to 31 in the same manner.
- Feature dimension is constant 384 for all blocks used, channel-mix expansion factor is 4 and there is no expansion factor in time mix. 
- Dropout of 0.15 in each block.

#### Speech-to-text alignment framework

We train the speech-to-text alignments jointly with the decoder by using adaptation of unsupervised alignment algorithm which was proposed in the [implementation of Fastpitch](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/FastPitch). 
- This aligner encodes text and mel-spectrogram using <span style="background-color: #FFFF00">[1-D convolutions](#tbd)</span> and projects them to a space with same dimensionality.
- The "soft" alignment is computed using a <span style="background-color: #FFFF00">[pair-wise L<sub>2</sub>](#tbd)</span> distance of the encoded text and mel representations, then <span style="background-color: #FFFF00">[Connectionist Temporal Classification (CTC)](#tbd)</span> loss is used to learn these alignments.
- To obtain `monotonic binarized alignments` (i.e. `"ground truth"` durations), the `Viterbi Algorithm` is used to find the most likely monotonic path. 

#### Extended Mixer-TTS

- For the extended model - [ALBERT](https://arxiv.org/abs/1909.11942) - 2019 (from [HuggingFace](https://arxiv.org/abs/1910.03771) - 2019) pretrained on large corpus of English text is used.
- We kept the LM tokenization method and utilized the frozen embeddings for input tokens.
- The lengths of original and tokenized text from external LM are different because they are produced by different tokenizers. To align two sequences, we use a <span style="background-color: #FFFF00">[single head self-attention block](#tbd)</span> applied to LM embeddings *lm<sub>emb</sub>* and encoder output *t<sub>e</sub>*, which mixed their features while preserving the lengths of `basic` text embeddings.
- Text features for self-attention aligning are extracted with convolutional layers preceded by separate positional embedding layers.

## Results

We evaluate the quality of proposed models combined with a HiFi-GAN <span style="background-color: #FFFF00">[vocoder](#tbd)</span> on LJSpeech.

Dataset used: [`LJSpeech dataset`](https://keithito.com/LJ-Speech-Dataset/) ~ 2.6 GB
- This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.
- MOS of 4.27 <span style="background-color: #FFFF00">source unknown</span>
- This dataset was split into 3 sets:
    1. 12, 500 samples for training
    2. 100 samples for training
    3. 500 samples for testing
    - The text was lower-cased and all punctuation was left intact.

Experimented with two tokenization approaches:
1. character-based
2. phoneme-based
    - For grapheme-to-phoneme conversion we used [ARPABET representation in the CMUdict vocabulary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict) and left ambiguous words and heteronyms in character representation.

Other training details:
- We converted ground truth <span style="background-color: #FFFF00">[22050Hz sampling rate audios](#tbd)</span> to mel-spectrograms using a <span style="background-color: #FFFF00">[Short-Time Fourier Transform (STFT)](#tbd)</span> with 50ms <span style="background-color: #FFFF00">[Hann window](#tbd)</span> and 12.5 ms <span style="background-color: #FFFF00">[frame hop](#tbd)</span>.
- Ground truth pitch was extracted using the [librosa library](https://dawenl.github.io/publications/McFee15-librosa.pdf) with values-aligned along mel-spectrogram frames.
- The model was trained for 1000 epochs using the  [LAMB optimizer](https://arxiv.org/abs/1904.00962) with *&beta;<sub>1</sub>* = 0.9, *&beta;<sub>2</sub>* = 0.98, &epsilon; = 10<sup>-8</sup>, a weight decay of 10<sup>-6</sup> and gradient clipping of 1000.0.
- A <span style="background-color: #FFFF00">[Noam annealing learning rate policy](#tbd)</span> was used with a learning rate of 0.1 and a 1000 steps warmup. 
- We used a total batch of 128 for 4 GPUs with gradients accumulation of 2. The training takes around 12 hours on 4xV100 GPUs in mixed precision mode. 

<i>And finally, this was the first paper I reviewed in the text-to-speech domain, ended up with more questions than answers. Looking forward to learning more as I go along this journey. Feel free to drop interesting papers in this domain in the comments section!</i>