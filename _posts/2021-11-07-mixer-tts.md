---
layout: post
current: post
cover: 'assets/images/text-to-speech.jpg'
navigation: true
title: 'Mixer-TTS'
date: 2021-11-07 12:0:0
tags: DL Conversational-AI TTS Model-Architecture
class: post-template
subclass: 'post'
author: aroraakshit
---

# `Mixer-TTS: non-autoregressive, fast and compact text-to-speech model conditioned on language model embeddings` 
### by [Oktai Tatanov](https://ru.linkedin.com/in/oktai15), [Stanislav Beliaev](https://www.linkedin.com/in/stasbel), [Boris Ginsburg](https://www.linkedin.com/in/boris-ginsburg-2249545) (NVIDIA, Santa Clara).

Available on [arXiv](https://arxiv.org/abs/2110.03584) - Oct 2021. Code not available yet. 
This journal was last updated on 11/8/2021.

Note: The views and opinions expressed on this website are those of my own and do not represent those of my employer, NVIDIA.

## Abstract

This paper describes `Mixer-TTS`, a [`non-autoregressive`](#tbd) model for [`mel-spectrogram`](#tbd) generation. The idea here is that `Mixer-TTS` achieves similar quality (measured as [`Mean Opinion Score (MOS)`](#tbd)) model as state of the art TTS models today, like [`FastPitch`](https://arxiv.org/abs/2006.06873), using almost half the parameters, and therefore better inference performance. Basic `Mixer-TTS` contains `pitch` and `duration` predictors as well.

## Notes

### Intuition behind Mixer-TTS:
1. Latest improvements in training and inference speed has been mostly related to switching from sequential, autoregressive models (like [`Tacotron`](https://arxiv.org/abs/1703.10135) - 2017, [`Wavenet`](https://arxiv.org/abs/1712.05884) - 2017 and [`Deep Voice 3`](https://arxiv.org/abs/1710.07654) - 2017) to parallel, non-autoregressive models (like [`FastSpeech`](https://arxiv.org/abs/1905.09263) - 2019, [`FastSpeech 2`](https://arxiv.org/abs/2006.04558) - 2020, [`Fastpitch`](https://arxiv.org/abs/2006.06873) - 2020 and [`Talknet`](https://arxiv.org/pdf/2005.05514.pdf) - 2020).
    - Non-autoregressive models generate speech two order of magnitude faster than auto-regressive models with similar quality. 
    - <span style="background-color: #FFFF00">unknown meaning of autoregressive vs non-autoregressive</span>
    - Example, FastPitch generates mel-spectrograms 60x faster than Tacotron 2.
2. The robustness of TTS models has been improved by using an explicit duration predictor (like in [`Deep Voice`](https://arxiv.org/abs/1702.07825) - 2017, [`Deep Voice 2`](https://arxiv.org/abs/1705.08947) - 2017, [`FastSpeech`](https://arxiv.org/abs/1905.09263) - 2019, [`FastSpeech 2`](https://arxiv.org/abs/2006.04558) - 2020, [`Fastpitch`](https://arxiv.org/abs/2006.06873) - 2020 and [`Talknet`](https://arxiv.org/pdf/2005.05514.pdf) - 2020)
    - Traditionally, models with duration predictors have been trained in a supervised manner with external ground truth alignments.
    - For example, TalkNet used the alignment from auxiliary ASR models, while FastSpeech and FastPitch used alignments from a teacher TTS model.
    - [`Glow-TTS`](https://arxiv.org/abs/2005.11129) - 2020 proposed a flow based algorithm for unsupervised alignment training, further improved in [`RAD-TTS`](https://openreview.net/pdf?id=0NQwnnwAORi) - 2021 and modified for non-autoregressive models [here - `One TTS Alignment To Rule Them All`](https://arxiv.org/abs/2108.10447) - 2021 - and that is used in Mixer-TTS model.
3. The quality of non-autoregressive models improved with:
    - [`Fastpitch`](https://arxiv.org/abs/2006.06873) - 2020 adding a pitch predictor for [`fundamental frequency (F0)`](#tbd). 
    - Further improvement suggested by [`Hayashi et al`](https://www.isca-speech.org/archive/interspeech_2019/hayashi19_interspeech.html) - 2019 was to augment TTS model with input representation from pre-trained [`BERT`](https://arxiv.org/abs/1810.04805) - 2018 language model.
        - Intuition here is that text embeddings contain information about the importance of each word, which helped to improve speech prosody and pronunciation.
    - The usage of semantic context for TTS was extended in [`Xu et al`](https://arxiv.org/abs/2011.05161) - 2021
4. The model backbone is based on [`MLP-Mixer`](https://arxiv.org/abs/2105.01601) - 2021 architecture from computer vision adapted for speech. 
    - MLP-Mixer makes the model significantly smaller and faster than Transformer-based TTS models (like [`FastSpeech 2`](https://arxiv.org/abs/2006.04558) - 2020, [`Fastpitch`](https://arxiv.org/abs/2006.06873) - 2020) <span style="background-color: #FFFF00">not sure why, yet</span>
5. Using token embeddings is significantly less expensive inferring BERT outputs as shown in [`Hayashi et al`](https://www.isca-speech.org/archive/interspeech_2019/hayashi19_interspeech.html) - 2019. <span style="background-color: #FFFF00">Unclear intuition here</span>.
    - They notably improve speech quality with a very modest increase in the model size and inference speed.

There are two versions of Mixer-TTS presented in the paper:
1. `basic` - uses <span style="background-color: #FFFF00">unclear about the implementation here - perhaps similar to Fastpitch</span> embeddings. Achieves a MOS of 4.05. About 19.2 M parameters.
2. `extended` - uses `token embeddings` from pre-trained language model (ALBERT from HuggingFace). Achieves a MOS of 4.11. About 24M parameters.

Samples published here: [mixer-tts.github.io](https://mixer-tts.github.io/)

### Architecture 
Here is what the model architecture looks like:

![figure:1](assets/images/2021-11-07-mixer-tts/f1.png)

We encode the text and align it by using audio features in a separate module to get "ground truth" durations. Then we calculate character or `phoneme-level` pitch values and feed them all into the length regulator module to expand each character or phoneme feature along with their corresponding durations. Next, the decoder generates mel-spectrogram from the encoded representations.

Mixer-TTS is structurally very similar to FastPitch with two major changes:
1. Replace all feed-forward Transformer-based blocks in the encoder and decoder with new Mixer-TTS blocks.
2. Use an unsupervised speech-to-text alignment framework to train duration predictor. However, duration and pitch predictor architectures are the same as FastPitch. 
3. The extended Mixer-TTS additionally includes conditioning on embeddings from pretrained LM.

Loss function - aligner loss and mean-squared errors between ground truth and predicted values for mel-spectrogram, duration and pitch: \\
L = L<sub>aligner</sub> + L<sub>mel</sub> + 0.1 * L<sub>durs</sub> + 0.1 * L<sub>pitch</sub>

#### Mixer-TTS block



#### Speech-to-text alignment framework



#### Extended Mixer-TTS



## Results

Dataset used: [`LJSpeech dataset`](https://keithito.com/LJ-Speech-Dataset/) ~ 2.6 GB
- MOS of 4.27 <span style="background-color: #FFFF00">source unknown</span>
- This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.
- The texts were published between 1884 and 1964, and are in the public domains The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain.

